{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: \n",
    "\n",
    "Student Netid:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Naive Bayes (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. From your reading you know that the naive Bayes classifier works by calculating the conditional probabilities of each feature, $e_i$, occuring with each class $c$ and treating them independently. This results in the probability of a certain class occuring given a set of features, or a piece of evidence, $E$, as\n",
    "\n",
    "$$P(c \\mid E) = \\frac{p(e_1 \\mid c) \\cdot p(e_2 \\mid c) \\cdot \\cdot \\cdot p(e_k \\mid c) \\cdot p(c)}{p(E)}.$$\n",
    "\n",
    "The conditional probability of each piece of evidence occuring with a given class is given by\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c)}{\\text{count}(c)}.$$\n",
    "\n",
    "In the above equation $\\text{count}(e_i, c)$ is the number of documents in a given class that contain feature $e_i$ and $\\text{count}(c)$ is the number of documents that belong to class $c$. \n",
    "\n",
    "A common variation of the above is to use Laplace (sometimes called +1) smoothing. Recall the use of Laplace smoothing introduced toward the end of Chapter 3 in the section Probability Estimation. This is done in sklearn by setting `alpha=1` in the `BernoulliNB()` function (this is also the default behavior). The result of Laplace smoothing will slightly change the conditional probabilities,\n",
    "\n",
    "$$P(e_i \\mid c) = \\frac{\\text{count}(e_i, c) + 1}{\\text{count}(c) + 2}.$$\n",
    "\n",
    "In no more than **one paragraph**, describe why this is useful, and use the bias-variance tradeoff to justify its use. Try to think of a case when not using Laplace smoothing would result in \"bad\" models. Try to give an example. Be precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The useful point of Laplace smoothing is to change the zero-values of your data into small positive number to prevent fail your entire process.  When you change the zero value, you should pay attention to reduce other values and the total sum of probability maintains 1 at the same time.\n",
    "Considering,\n",
    "y_pred=argmaxP(y)P(x_1|y)P(x_2|y)···P(x_n|y)for instance\n",
    "Step1\n",
    "First, you had 30 spam emails and 70 non-spam emails, then you hadP(spam) =0.3,P(non−spam)=0.7···(∗1)\n",
    "Now,maintaining the total sum of probability is1, \n",
    "changes to 31 spam emailsand 69 non-spam emails.P(spam) = 0.31,P(non−spam) = 0.69···(∗2)\n",
    "(∗1)and(∗2)belong to P(y) and the latter value should be decreased considering the augmentation in the process of (∗3)→(∗4)\n",
    "Step2\n",
    "Think about P(x1|y).\n",
    "Assume you originally had ’buy’ in 10 out of 40 spam emails and in no non-spam-emails,\n",
    "P(buy|spam) =0.25,P(buy|non−spam) = 0···(∗3)\n",
    "”P(buy|non−spam) = 0” leads the entire process to fail by being multipliedsince the value is zero. \n",
    "That is, ypred=argmaxP(y)P(buy|non−spam)P(x_2|y)···P(x_n|y) = 0 \n",
    "So,this  phenomenon  yields  this  model  bad.   To  prevent  this  disadvantageous process, you should use Laplace smoothing. Based  on  this  concept, I try to change the zero value into the small positive number.  \n",
    "To change, suppose you had ’buy’ in 12 out of 41 spamemails and in 2 out of 200 spam emails.\n",
    "P(buy|spam) = 0.29,P(buy|non−spam) = 0.01···(∗4)\n",
    "y_pred = argmaxP(y)P(buy|non−spam)P(x_2|y)···P(x_n|y) != 0 \n",
    "You can hamper the entire-process from failing by the Laplace smoothing.  \n",
    "In specifically,\n",
    "Suppose P_{emperical}=x_i/N= 0, then you should employ the Laplace smoothing like this \n",
    "P_{α−smoothed}=xi/{α + αd} where d is constant.\n",
    "You could see that α varies and there is tradeoff between variance and bias:\n",
    "If α is small, it leads to high variance.\n",
    "If α is large, it leads to high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<wand.image.Image: 53498fc 'PDF' (612x792)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wand.image import Image as WImage\n",
    "img = WImage(filename='ds_1001_hw5_1.pdf')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Text classification for sentiment analysis (20 Points)\n",
    "For this part of the assignment, we are going to use a data set of movie ratings from IMDB.com. The data consists of the text of a movie review and a target variable which tells us whether the reviewer had a positive feeling towards the movie (equivalent to rating the movie between 7 and 10) or a negative feeling (rating the movie between 1 and 4). Neutral reactions are not included in the data.\n",
    "\n",
    "The first column is the review text; the second is the text label 'P' for positive or 'N' for negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 (1 Point) \\. Load the data into a pandas `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'One of the first of the best musicals Anchors...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Visually disjointed and full of itself the di...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'These type of movies about young teenagers st...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'I would rather of had my eyes gouged out with...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The title says it all. Tail Gunner Joe was a ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>'Alright friends a serious movie buff is expec...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>'I found this film embarrassing to watch. I fe...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>'To put it simply I am not fond of westerns. A...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>'Some of these viewer comments are just ridicu...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>'Sometimes a premise starts out good but becau...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Class\n",
       "0     'One of the first of the best musicals Anchors...     P\n",
       "1     'Visually disjointed and full of itself the di...     N\n",
       "2     'These type of movies about young teenagers st...     P\n",
       "3     'I would rather of had my eyes gouged out with...     N\n",
       "4     'The title says it all. Tail Gunner Joe was a ...     N\n",
       "...                                                 ...   ...\n",
       "8495  'Alright friends a serious movie buff is expec...     N\n",
       "8496  'I found this film embarrassing to watch. I fe...     N\n",
       "8497  'To put it simply I am not fond of westerns. A...     N\n",
       "8498  'Some of these viewer comments are just ridicu...     N\n",
       "8499  'Sometimes a premise starts out good but becau...     N\n",
       "\n",
       "[8500 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "imdv_data=pd.read_csv('imdb.csv',header=0, sep=',') \n",
    "imdv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1434    'Although I have to admit I laughed more watch...\n",
       " 4644    'If your idea of a thriller is car chases expl...\n",
       " 2017    'If you want to learn something about the Span...\n",
       " 2744    'This documentary was interesting but it was a...\n",
       " 725     'What a dreadful movie! For some reason scient...\n",
       "                               ...                        \n",
       " 2015    'I resisted seeing this movie and I understand...\n",
       " 3891    'Largely dense road movie with some comic reli...\n",
       " 7847    'I must admit at first I wasnt expecting anyth...\n",
       " 2191    'Over-powered mobile suits that can annihilate...\n",
       " 7473    'Some of those guys that watch films and compl...\n",
       " Name: Text, Length: 6375, dtype: object,\n",
       " 2417    'When my now college age daughter was in presc...\n",
       " 1062    'After watching this film I was left with a tw...\n",
       " 3954    'If you are studying Welles and want to see ju...\n",
       " 6902    'Question: how does a bourgeois director treat...\n",
       " 6       'National Lampoon Goes to the Movies (1981) is...\n",
       "                               ...                        \n",
       " 266     'This first-rate western tale of the gold rush...\n",
       " 1069    'I became a fan of the TV series `Homicide: Li...\n",
       " 4603    'It SURPRISINGLY had a plot! ;) Ive seen movie...\n",
       " 6794    'Dog days is one of most accurate films ive ev...\n",
       " 78      'Holes (2003 Dir Andrew Davis)  When Stanley Y...\n",
       " Name: Text, Length: 2125, dtype: object,\n",
       " 1434    N\n",
       " 4644    P\n",
       " 2017    P\n",
       " 2744    P\n",
       " 725     N\n",
       "        ..\n",
       " 2015    P\n",
       " 3891    N\n",
       " 7847    P\n",
       " 2191    P\n",
       " 7473    P\n",
       " Name: Class, Length: 6375, dtype: object,\n",
       " 2417    P\n",
       " 1062    N\n",
       " 3954    N\n",
       " 6902    N\n",
       " 6       N\n",
       "        ..\n",
       " 266     P\n",
       " 1069    P\n",
       " 4603    N\n",
       " 6794    P\n",
       " 78      P\n",
       " Name: Class, Length: 2125, dtype: object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Code here\n",
    "#df.[‘column_name’]\n",
    "imdv_data.rename(columns={\"Text\": \"X\", \"Class\": \"Y\"})\n",
    "from sklearn.model_selection import train_test_split\n",
    "imdv_train, imdv_test = train_test_split(imdv_data,train_size=0.75, test_size=None)\n",
    "imdv_train,imdv_test\n",
    "X_train=imdv_train[\"Text\"]\n",
    "X_test=imdv_test[\"Text\"]\n",
    "Y_train=imdv_train[\"Class\"]\n",
    "Y_test=imdv_test[\"Class\"]\n",
    "X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1434    0.0\n",
       "4644    1.0\n",
       "2017    1.0\n",
       "2744    1.0\n",
       "725     0.0\n",
       "       ... \n",
       "2015    1.0\n",
       "3891    0.0\n",
       "7847    1.0\n",
       "2191    1.0\n",
       "7473    1.0\n",
       "Name: Class, Length: 6375, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_num=Y_train.replace(\"P\", 1).replace(\"N\",0)\n",
    "Y_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3406    1.0\n",
       "2756    0.0\n",
       "2256    1.0\n",
       "622     1.0\n",
       "3939    0.0\n",
       "       ... \n",
       "5426    1.0\n",
       "6911    0.0\n",
       "3794    0.0\n",
       "7691    0.0\n",
       "7082    0.0\n",
       "Name: Class, Length: 2125, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_num=Y_test.replace(\"P\", 1).replace(\"N\",0)\n",
    "Y_test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (1 Point)\\. Code the target variable to be numeric: use the value `1` to represent 'P' and `0` to represent 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'One of the first of the best musicals Anchors...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Visually disjointed and full of itself the di...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'These type of movies about young teenagers st...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'I would rather of had my eyes gouged out with...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The title says it all. Tail Gunner Joe was a ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>'Alright friends a serious movie buff is expec...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>'I found this film embarrassing to watch. I fe...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>'To put it simply I am not fond of westerns. A...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>'Some of these viewer comments are just ridicu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>'Sometimes a premise starts out good but becau...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      X    Y\n",
       "0     'One of the first of the best musicals Anchors...  1.0\n",
       "1     'Visually disjointed and full of itself the di...  0.0\n",
       "2     'These type of movies about young teenagers st...  1.0\n",
       "3     'I would rather of had my eyes gouged out with...  0.0\n",
       "4     'The title says it all. Tail Gunner Joe was a ...  0.0\n",
       "...                                                 ...  ...\n",
       "8495  'Alright friends a serious movie buff is expec...  0.0\n",
       "8496  'I found this film embarrassing to watch. I fe...  0.0\n",
       "8497  'To put it simply I am not fond of westerns. A...  0.0\n",
       "8498  'Some of these viewer comments are just ridicu...  0.0\n",
       "8499  'Sometimes a premise starts out good but becau...  0.0\n",
       "\n",
       "[8500 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdv_data.rename(columns={\"Text\": \"X\", \"Class\": \"Y\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (2 Points)\\. Put all of the text into a data frame called `X` and the target variable in a data frame called `Y`. Make a train/test split where you give 75% of the data to training. Feel free to use any function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "imdv_train, imdv_test = train_test_split(imdv_data,train_size=0.75, test_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (5 Points)\\. Create a binary `CountVectorizer()` and a binary `TfidfVectorizer()`. Use the original single words as well as bigrams (in the same model). Also, use an \"english\" stop word list. Fit these to the training data to extract a vocabulary and then transform both the train and test data. Hint - look at the API documentation for both vectorizers to see what we mean by \"binary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ahn-\n",
      "[nltk_data]     eunjoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer1 = CountVectorizer(ngram_range=(0, 1),\n",
    "                                  token_pattern=r'\\b\\w+\\b', min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_11 = bigram_vectorizer1.fit_transform(X_train).toarray()\n",
    "X_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "nan_array = np.isnan(X_11)\n",
    "not_nan_array = ~ nan_array\n",
    "array1 = X_11[not_nan_array]\n",
    "array1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286875000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 0. 0. ... 0. 1. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dab301dfa4af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0marray1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    797\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 0. 0. ... 0. 1. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(array2 ,array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>577092</th>\n",
       "      <th>577093</th>\n",
       "      <th>577094</th>\n",
       "      <th>577095</th>\n",
       "      <th>577096</th>\n",
       "      <th>577097</th>\n",
       "      <th>577098</th>\n",
       "      <th>577099</th>\n",
       "      <th>577100</th>\n",
       "      <th>577101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6375 rows × 577102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1       2       3       4       5       6       7       8       \\\n",
       "0          0       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          0       0       0       0       0       0       0       0       0   \n",
       "3          0       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6370       0       0       0       0       0       0       0       0       0   \n",
       "6371       0       0       0       0       0       0       0       0       0   \n",
       "6372       0       0       0       0       0       0       0       0       0   \n",
       "6373       0       0       0       0       0       0       0       0       0   \n",
       "6374       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "      9       ...  577092  577093  577094  577095  577096  577097  577098  \\\n",
       "0          0  ...       0       0       0       0       0       0       0   \n",
       "1          0  ...       0       0       0       0       0       0       0   \n",
       "2          0  ...       0       0       0       0       0       0       0   \n",
       "3          0  ...       0       0       0       0       0       0       0   \n",
       "4          0  ...       0       0       0       0       0       0       0   \n",
       "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6370       0  ...       0       0       0       0       0       0       0   \n",
       "6371       0  ...       0       0       0       0       0       0       0   \n",
       "6372       0  ...       0       0       0       0       0       0       0   \n",
       "6373       0  ...       0       0       0       0       0       0       0   \n",
       "6374       0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "      577099  577100  577101  \n",
       "0          0       0       0  \n",
       "1          0       0       0  \n",
       "2          0       0       0  \n",
       "3          0       0       0  \n",
       "4          0       0       0  \n",
       "...      ...     ...     ...  \n",
       "6370       0       0       0  \n",
       "6371       0       0       0  \n",
       "6372       0       0       0  \n",
       "6373       0       0       0  \n",
       "6374       0       0       0  \n",
       "\n",
       "[6375 rows x 577102 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_11_data=pd.DataFrame(X_11)\n",
    "X_11_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer3 = CountVectorizer(ngram_range=(0, 1),\n",
    "                                  token_pattern=r'\\b\\w+\\b', min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_12 = bigram_vectorizer1.fit_transform(X_train).toarray()\n",
    "X_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(X_11,Y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mylogistic=logistic_regression.fit(X_11,Y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c6af4d8736ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_11_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "new_data=np.concatenate([X_11_data, Y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_data=pd.DataFrame(X_1)\n",
    "X_1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2=bigram_vectorizer1.fit_transform(B).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer2 = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                  token_pattern=r'\\b\\w+\\b', min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3 = bigram_vectorizer2.fit_transform(A).toarray()\n",
    "X_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4 = bigram_vectorizer2.fit_transform(B).toarray()\n",
    "X_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<107x37 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 43 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer(stop_words='english')\n",
    "X_train_tran = vectorizer1.fit_transform(A)\n",
    "X_train_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<192x126 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer1 = TfidfVectorizer()\n",
    "X_test_tran = vectorizer1.fit_transform(B)\n",
    "X_test_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<192x126 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2=CountVectorizer()\n",
    "X_train_tran_CV = vectorizer2.fit_transform(B)\n",
    "X_train_tran_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-6d3676042c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_tran_CV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_tran_CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer2=CountVectorizer()\n",
    "X_train_tran_CV = vectorizer2.fit_transform(A)\n",
    "X_train_tran_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 (6 Points)\\. Create `LogisticRegression()` and `BernoulliNB()` models. For all settings, keep the default values. In a single plot, show the AUC curve for both classifiers and both vectorizers defined above. In the legend, include the area under the ROC curve (AUC). Do not forget to label your axes. Your final plot will be a single window with 4 curves.\n",
    "\n",
    "Which model do you think does a better job? Why? Explain in no more than a paragraph.\n",
    "\n",
    "Extra credit (2 points): Do any of the options perform identically? If so, can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this so your plots show properly\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 148)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6375,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-feb5d0ec6590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tran\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     96\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(X_train_tran,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 117)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5\\. Use the model from question 4 that you think did the best job and predict the rating of the test data. Find 5 examples were labeled positive, but were incorrectly classified as negative. Print out the reviews below and include an explanation as to why you think it may have been incorrectly classified. You can pick any 5. They do not have to be at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here to display 5 incorrect reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation for the 5 reviews chosen here!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
